{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: SVM and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "In this assignment, we’ll be working with natural language data. In particular, we’ll be doing sentiment analysis on movie reviews. This problem will give you the opportunity to try your hand at feature engineering, which is one of the most important parts of many data science problems. From a technical standpoint, this homework has two new pieces. First, you’ll be implementing Pegasos. Pegasos is essentially stochastic subgradient descent for the SVM with a particular schedule for the step-size. Second, because in natural langauge domains we typically have huge feature spaces, we work with sparse representations of feature vectors, where only the non-zero entries are explicitly recorded. This will require coding your gradient and SGD code using hash tables (dictionaries in Python), rather than numpy arrays. We begin with some practice with subgradients and an easy problem that introduces the Perceptron algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Calculating Subgradients\n",
    "Recall that a vector $g\\in\\Re^{d}$ is a $\\textbf{subgradient}$ of\n",
    "$f:\\Re^{d}\\to\\Re$ at $x$ if for all $z$, \n",
    "$$\n",
    "f(z)\\ge f(x)+g^{T}(z-x).\n",
    "$$\n",
    "As we noted in lecture, there may be $0$, $1$, or infinitely many\n",
    "subgradients at any point. The $\\textbf{subdifferential}$ of $f$ at\n",
    "a point $x$, denoted $\\partial f(x)$, is the set of all subgradients\n",
    "of $f$ at $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Subgradients for pointwise maximum of functions. Suppose \n",
    "$f_{1},\\ldots,f_{m}:\\Re^{d}\\to\\Re$\n",
    "are convex functions, and\n",
    "\n",
    "$$\n",
    "f(x)=\\max_{i=1,\\ldots,,m}f_{i}(x).\n",
    "$$\n",
    "\n",
    "Let $k$ be any index for which $f_{k}(x)=f(x)$, and choose $g\\in\\partial f_{k}(x)$.\n",
    "We are using the fact that a convex function on $\\Re^{d}$ has a non-empty subdifferential at all points. Show that $g\\in\\partial f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: We can find $f(x) = f_{k}(x)$ for any exsiting $k$, let $g\\in\\partial f_{k}(x)$ at point $x$, and any $f_k(z)\\ge f_k(x)+g^{T}(z-x)$, so $f(z) \\ge f_k(z) \\ge f_k(x)+g^{T}(z-x)$, so $g$ must be $g\\in\\partial f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Subgradient of hinge loss for linear prediction. Give a subgradient of\n",
    "\n",
    "$$\n",
    "J(w)=\\max\\left\\{ 0,1-yw^{T}x\\right\\} .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: compute the subgradient of hinge loss function\n",
    "$$\n",
    "\\partial J(w)= \n",
    "\\begin{cases}\n",
    "  -1 & \\text{if $yw^{T}x < 1$} \\\\\n",
    "  0 & \\text{if $yw^{T}x > 1$} \\\\\n",
    "  [-1,0] & \\text{if $yw^{T}x = 1$}\n",
    "\\end{cases}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Perceptron\n",
    "\n",
    "The perceptron algorithm is often the first classification algorithm\n",
    "taught in machine learning classes. Suppose we have a labeled training\n",
    "set $\\left(x_{1},y_{1}\\right),\\ldots,(x_{n},y_{n})\\in\\Re^{d}\\times\\left\\{ -1,1\\right\\} $.\n",
    "In the perceptron algorithm, we are looking for a hyperplane that\n",
    "perfectly separates the classes. That is, we're looking for $w\\in\\Re^{d}$\n",
    "such that\n",
    "\n",
    "$$\n",
    "y_{i}w^{T}x_{i}>0\\;\\forall i\\in\\left\\{ 1,\\ldots,n\\right\\} .\n",
    "$$\n",
    "\n",
    "Visually, this would mean that all the $x$'s with label $y=1$ are\n",
    "on one side of the hyperplane $\\left\\{ x\\mid w^{T}x=0\\right\\} $,\n",
    "and all the $x's$ with label $y=-1$ are on the other side. When\n",
    "such a hyperplane exists, we say that the data are $\\textbf{linearly\n",
    "separable}$. The perceptron algorithm is given in Algorithm\n",
    "1\n",
    "![perceptron](perceptron.png).\n",
    "\n",
    "There is also something called the $\\textbf{perceptron loss,}$ given by \n",
    "\n",
    "$$\n",
    "\\ell(\\hat{y},y)=\\max\\left\\{ 0,-\\hat{y}y\\right\\} .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Show that if $\\left\\{ x\\mid w^{T}x=0\\right\\} $ is a separating hyperplane\n",
    "for a training set $D=\\left(\\left(x_{1},y_{1}\\right),\\ldots,(x_{n},y_{n})\\right)$,\n",
    "then the average perceptron loss on $D$ is $0$. Thus any separating\n",
    "hyperplane of $D$ is an empirical risk minimizer for perceptron\n",
    "loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: if $\\left\\{ x\\mid w^{T}x=0\\right\\} $ is a separating hyperplane\n",
    "for a training set $D=\\left(\\left(x_{1},y_{1}\\right),\\ldots,(x_{n},y_{n})\\right)$,\n",
    "we always have $y_{i}w^{T}x_{i}>0\\;\\forall i\\in\\left\\{ 1,\\ldots,n\\right\\}$, \n",
    "which means that $\\hat{y}y > 0$ always. \n",
    "Then the perceptron loss for each training example is $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let $H$ be the linear hypothesis space consisting of functions\n",
    "$x\\mapsto w^{T}x$. Consider running stochastic subgradient descent\n",
    "(SSGD) to minimize the empirical risk with the perceptron loss. We'll\n",
    "use the version of SSGD in which we cycle through the data points\n",
    "in each epoch. Show that if we use a fixed step size $1$, we terminate\n",
    "when our training data are separated, and we make the right choice\n",
    "of subgradient,  then we are exactly doing the Perceptron algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The loss function \n",
    "$$\n",
    "J(w) = \\max\\left\\{ 0,-\\hat{y}y\\right\\}\n",
    "\\begin{cases}\n",
    "  0 & \\hat{y}y>0 \\\\\n",
    "  -\\hat{y}y & \\hat{y}y<0\n",
    "\\end{cases}\n",
    "$$\n",
    "When trying to run stochatic subgradent descent, update gradient for each example, where $\\alpha=1$ as step size.\n",
    "$$ w = w - \\alpha * \\partial J(w) = \n",
    "\\begin{cases}\n",
    "  w & \\hat{y}y>0 \\\\\n",
    "  w + \\alpha y_i x_i & \\hat{y}y<0\n",
    "\\end{cases}$$\n",
    "Then we can see that it is the same as perception algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Suppose the perceptron algorithm returns $w$. Show that $w$ is a\n",
    "linear combination of the input points. That is, we can write $w=\\sum_{i=1}^{n}\\alpha_{i}x_{i}$\n",
    "for some $\\alpha_{1},\\ldots,\\alpha_{n}\\in\\Re$. The $x_{i}$ for\n",
    "which $\\alpha_{i}\\neq0$ are called support vectors. Give a characterization\n",
    "of points that are support vectors and not support vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: In the training, if current $w^T x$ cannot classify correctly for one example, we will add $y_i x_i$ to the w, until we classify all the examples correctly. Thus $w$ will the linear combination of $x_i$. During training, the training examples that classify incorrectly are considered as support vectors, and the ones that classify correctly are consider as non support vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 The Data\n",
    "\n",
    "We will be using the [Polarity Dataset v2.0](https://www.cs.cornell.edu/people/pabo/movie-review-data), constructed by Pang and Lee. It has the full text from 2000 movies\n",
    "reviews: 1000 reviews are classified as ``positive`` and 1000 as\n",
    "``negative.`` Our goal is to predict whether a review has positive\n",
    "or negative sentiment from the text of the review. Each review is\n",
    "stored in a separate file: the positive reviews are in a folder called\n",
    "``pos``, and the negative reviews are in ``neg``. We have provided\n",
    "some code in $\\texttt{load.py}$ to assist with reading these files.\n",
    "You can use the code, or write your own version. The code removes\n",
    "some special symbols from the reviews. Later you can check if this\n",
    "helps or hurts your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load all the data and randomly split it into 1500 training examples and 500 validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from load import shuffle_data\n",
    "# # Save data into 'save.p' file\n",
    "# shuffle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dataset = pickle.load(open(\"save.p\", \"rb\"))\n",
    "\n",
    "num_train = 1500\n",
    "train_examples = dataset[:num_train]\n",
    "valid_examples = dataset[num_train:]\n",
    "# print(len(train_examples))\n",
    "# print(len(valid_examples))\n",
    "# print(train_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Sparse Representations\n",
    "\n",
    "The most basic way to represent text documents for machine learning\n",
    "is with a ``bag-of-words`` representation. Here every possible word\n",
    "is a feature, and the value of a word feature is the number of times\n",
    "that word appears in the document. Of course, most words will not\n",
    "appear in any particular document, and those counts will be zero.\n",
    "Rather than store a huge number of zeros, we use a sparse representation,\n",
    "in which we only store the counts that are nonzero. The counts are\n",
    "stored in a key/value store (such as a dictionary in Python).\n",
    "For example, ``Harry Potter and Harry Potter II`` would be represented\n",
    "as the following Python dict: $\\text{x=\\{'Harry':2, 'Potter':2, 'and':1, 'II':1\\}}$.\n",
    "We will be using linear classifiers of the form $f(x)=w^{T}x$, and\n",
    "we can store the $w$ vector in a sparse format as well, such as $\\text{w=\\{'minimal':1.3, 'Harry':-1.1, 'viable':-4.2, 'and':2.2, 'product':9.1\\}}$.\n",
    "The inner product between $w$ and $x$ would only involve the features\n",
    "that appear in both $x$ and $w$, since whatever doesn't\n",
    "appear is assumed to be zero. For this example, the inner product\n",
    "would be $\\text{x[Harry] * w[Harry] + x[and] * w[and] = 2 * (-1.1) + 1 * (2.2)}$. \n",
    "To help you along, we've included two functions for working with\n",
    "sparse vectors: 1) a dot product between two vectors represented as\n",
    "dict's and 2) a function that increments one sparse vector by a scaled\n",
    "multiple of another vector, which is a very common operation. These\n",
    "functions are located in $\\text{util.py}$. It is\n",
    "worth reading the code, even if you intend to implement it yourself.\n",
    "You may get some ideas on how to make things faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function that converts an example (e.g. a list of words) into a sparse bag-of-words representation. You may find Python's Counter class to be useful here: [url](https://docs.python.org/2/library/collections.html). Note that a Counter is also a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 21, 'a': 17, 'and': 15, 'for': 11, 'of': 10, 'but': 10, 'to': 9, 'some': 8, 'have': 8, 'in': 8, 'is': 8, 'compensate': 6, 'you': 6, 'who': 5, 'this': 5, 'like': 5, 'an': 5, 'thornton': 4, 'cusack': 4, 'it': 4, 'that': 4, 'cast': 3, 'can': 3, 'pushing': 3, 'tin': 3, 'blanchett': 3, 'jolie': 3, 'has': 3, 'hip': 3, 'be': 3, 'film': 3, 'air': 3, 'with': 3, '!': 3, 'sometimes': 2, 'things': 2, 'are': 2, 'oh': 2, 'yes': 2, 'might': 2, 'not': 2, 'at': 2, 'people': 2, 'terrific': 2, 'score': 2, '?': 2, 'planes': 2, 'us': 2, 'how': 2, 'best': 2, 'i': 2, 'one': 2, 'so': 2, 'traffic': 2, 'controllers': 2, 'these': 2, 'falzone': 2, 'up': 2, 'boys': 2, 'will': 2, 'then': 2, \"doesn't\": 2, 'there': 2, 'wife': 2, 'last': 2, 'newell': 2, 'make': 2, \"they'll\": 2, 'minutes': 2, 'herself': 2, 'by': 2, 'fine': 2, 'actress': 2, 'his': 2, 'too': 2, 'stellar': 1, 'lot': 1, 'certainly': 1, 'features': 1, 'name': 1, 'stars': 1, 'going': 1, 'places': 1, 'billy': 1, 'bob': 1, 'cate': 1, 'angelina': 1, 'john': 1, 'realize': 1, 'first': 1, \"he's\": 1, 'actually': 1, 'veteran': 1, 'among': 1, 'quartet': 1, 'finelooking': 1, 'lackluster': 1, 'screen': 1, 'treatment': 1, 'idea': 1, 'comedy': 1, 'written': 1, 'all': 1, 'over': 1, 'workmanlike': 1, 'uninspired': 1, 'direction': 1, 'obnoxious': 1, 'would': 1, 'anyone': 1, 'tone': 1, 'deaf': 1, 'screaming': 1, 'exits': 1, 'clich': 1, 'd': 1, 'characterizations': 1, 'embarrassing': 1, 'joking': 1, 'situations': 1, 'etc': 1, \"don't\": 1, 'earthly': 1, 'from': 1, 'opening': 1, 'sequence': 1, 'big': 1, 'trouble': 1, 'squiggly': 1, 'quirky': 1, 'credits': 1, 'fakelooking': 1, 'passenger': 1, 'circling': 1, 'new': 1, 'york': 1, 'anne': 1, \"dudley's\": 1, 'inyourear': 1, 'music': 1, 'making': 1, 'wonder': 1, 'she': 1, 'ever': 1, 'got': 1, 'original': 1, 'nomination': 1, 'full': 1, 'monty': 1, 'let': 1, 'alone': 1, 'won': 1, \"wasn't\": 1, 'ready': 1, 'walk': 1, 'just': 1, 'yet': 1, 'quickly': 1, 'we': 1, 'descend': 1, 'into': 1, 'tightlyedited': 1, 'montage': 1, 'which': 1, 'screams': 1, 'large': 1, 'capital': 1, 'letters': 1, 'difficult': 1, 'job': 1, 'what': 1, 'their': 1, 'frantic': 1, 'mileaminute': 1, 'instructional': 1, 'personas': 1, 'juggling': 1, \"passenger's\": 1, 'lives': 1, 'huge': 1, 'real': 1, 'midair': 1, 'video': 1, 'game': 1, 'cool': 1, 'demonic': 1, 'auctioneer': 1, 'nick': 1, 'zone': 1, 'biz': 1, 'course': 1, 'until': 1, 'hipper': 1, 'cooler': 1, 'leatherclad': 1, 'flyboy': 1, 'assist': 1, 'guise': 1, 'russell': 1, 'bell': 1, 'shows': 1, 'challenge': 1, \"falzone's\": 1, 'finite': 1, 'space': 1, 'heavy': 1, 'duty': 1, 'testosterone': 1, 'starts': 1, 'exuding': 1, 'macho': 1, 'oneupmanship': 1, 'begins': 1, 'stop': 1, 'seeing': 1, 'juggle': 1, 'three': 1, '747s': 1, 'within': 1, \"cat's\": 1, 'whisker': 1, 'each': 1, 'other': 1, 'no': 1, 'broken': 1, 'hoop': 1, 'dreams': 1, 'wannaseehowfasticandrives': 1, 'ultimate': 1, 'showdown': 1, 'was': 1, 'my': 1, 'saw': 1, 'night': 1, 'director': 1, 'mike': 1, 'four': 1, 'weddings': 1, 'funeral': 1, 'must': 1, 'read': 1, 'different': 1, 'draft': 1, 'script': 1, 'because': 1, \"that's\": 1, 'being': 1, 'acted': 1, 'out': 1, 'between': 1, 'newark': 1, 'jfk': 1, 'la': 1, 'guardia': 1, 'ounce': 1, 'subtlety': 1, 'made': 1, 'awfully': 1, 'goodand': 1, 'funnymovies': 1, 'before': 1, 'antics': 1, 'cringe': 1, 'frown': 1, 'disbelief': 1, 'constantly': 1, 'looking': 1, 'your': 1, 'watch': 1, 'wait': 1, \"there's\": 1, 'still': 1, '100': 1, 'go': 1, \"film's\": 1, 'only': 1, 'saving': 1, 'grace': 1, 'whose': 1, 'connie': 1, 'spunky': 1, 'brash': 1, 'long': 1, 'island': 1, 'housewife': 1, 'wants': 1, 'better': 1, 'taking': 1, 'art': 1, 'classes': 1, 'wonderful': 1, 'accomplishment': 1, 'previously': 1, 'played': 1, 'redheaded': 1, 'australian': 1, 'gambler': 1, 'oscar': 1, 'lucinda': 1, 'tempestuous': 1, 'british': 1, 'monarch': 1, 'elizabeth': 1, \"she's\": 1, 'enough': 1, 'save': 1, 'picture': 1, 'looks': 1, 'performs': 1, 'solidly': 1, 'character': 1, 'joke': 1, 'as': 1, \"russell's\": 1, 'knock': 1, \"'em\": 1, 'dead': 1, \"isn't\": 1, 'bad': 1, 'upandcoming': 1, 'disappoints': 1, 'allowing': 1, 'displayed': 1, 'plaything': 1, 'cracks': 1, 'gum': 1, 'dons': 1, 'shades': 1, 'acts': 1, 'throughout': 1, 'everything': 1, 'else': 1, 'performance': 1, 'forced': 1, 'ten': 1, 'or': 1, 'inexplicable': 1, 'reason': 1, 'start': 1, 'coming': 1, 'together': 1, 'begin': 1, 'get': 1, 'sense': 1, 'been': 1, 'trailer': 1, 'teases': 1, \"it's\": 1, 'little': 1, 'late': 1, 'aside': 1, 'nothing': 1, 'more': 1, 'than': 1, 'embarrassment': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def convertDoc2Sparse(doc):\n",
    "    cnt = Counter()\n",
    "    for w in doc:\n",
    "        cnt[w] += 1\n",
    "    return cnt\n",
    "\n",
    "# Test doc sparse representation\n",
    "docSparseRep = convertDoc2Sparse(train_examples[0][:-1])\n",
    "print(docSparseRep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Support Vector Machine via Pegasos\n",
    "\n",
    "In this question you will build an SVM using the Pegasos algorithm.\n",
    "To align with the notation used in the Pegasos [Pegasos: Primal Estimated sub-GrAdient SOlver for SVM](http://ttic.uchicago.edu/~nati/Publications/PegasosMPB.pdf),\n",
    "we're considering the following formulation of the SVM objective function:\n",
    "\n",
    "$$\n",
    "\\min_{w\\in\\Re^{d}}\\frac{\\lambda}{2}\\|w\\|^{2}+\\frac{1}{m}\\sum_{i=1}^{m}\\max\\left\\{ 0,1-y_{i}w^{T}x_{i}\\right\\} .\n",
    "$$\n",
    "\n",
    "Note that, for simplicity, we are leaving off the unregularized bias\n",
    "term $b$. Pegasos is stochastic subgradient descent using a step\n",
    "size rule $\\eta_{t}=1/\\left(\\lambda t\\right)$. The pseudocode is\n",
    "given below:\n",
    "<img src=\"svm.png\" width=\"600\" height='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (Written). Consider the ``stochastic`` SVM objective function,\n",
    "which is the SVM objective function with a single training point (Recall that if $i$ is selected uniformly from the set $\\left\\{ 1,\\ldots,m\\right\\}$,\n",
    "then this stochastic objective function has the same expected value\n",
    "as the full SVM objective function.): $J_{i}(w)=\\frac{\\lambda}{2}\\|w\\|^{2}+\\max\\left\\{ 0,1-y_{i}w^{T}x_{i}\\right\\} $.\n",
    "The function $J_{i}(\\theta)$ is not differentiable everywhere. Give\n",
    "an expression for the gradient of $J_{i}(w)$ where it's defined,\n",
    "and specify where it is not defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\nabla J_{i}(w) & = & \\begin{cases}\n",
    "\\lambda w-y_{i}x_{i} & \\mbox{for }y_{i}w^{T}x_{i}<1\\\\\n",
    "\\lambda w & \\mbox{for }y_{i}w^{T}x_{i}>1. \\\\\n",
    "\\text{undefined} & \\mbox{for }y_{i}w^{T}x_{i}=1\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (Written) Show that a subgradient of $J_{i}(w)$ is given by \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "g & = & \\begin{cases}\n",
    "\\lambda w-y_{i}x_{i} & \\mbox{for }y_{i}w^{T}x_{i}<1\\\\\n",
    "\\lambda w & \\mbox{for }y_{i}w^{T}x_{i}\\ge1.\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "You may use the following facts without proof:\n",
    "1) If $f_{1},\\ldots,f_{m}:\\Re^{d}\\to\\Re$\n",
    "are convex functions and $f=f_{1}+\\cdots+f_{m}$, then $\\partial f(x)=\\partial f_{1}(x)+\\cdots+\\partial f_{m}(x)$.\n",
    "2) For $\\alpha\\ge0$, $\\partial\\left(\\alpha f\\right)(x)=\\alpha\\partial f(x)$.\n",
    "(Hint: Use the rules provided and the calculation in the first problem.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Since we have gradient at all points except $y_i w^T x_i=1$, so we only consider the subgradient at this point. The subgradient at $m=1$ in the hinge loss function $max\\{0,1-m\\}$ is $[-1,0]$, thus the subgradient for $\\partial J_{i}(w)$ can be $\\lambda w + k$ at $y_{i}w^{T}x_{i}=1$, where $k$ is in $[-1,0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (Written). Show that if your step size rule is $\\eta_{t}=1/\\left(\\lambda t\\right)$,\n",
    "then doing SGD with the subgradient direction from the previous problem\n",
    "is the same as given in the pseudocode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: If we use SGD with the subgradient direction, for each example $(x_i,y_i)$, the subgradient descent is $w = w - \\alpha * g$, where $g$ is subgradient is question 2, $\\alpha$ is step size as $\\eta_{t}=1/\\left(\\lambda t\\right)$. Thus the whole algorithm is the same as the given pseudocode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Implement the Pegasos algorithm to run on a sparse data representation.\n",
    "The output should be a sparse weight vector $w$. Note that our Pegasos\n",
    "algorithm starts at $w=0$. In a sparse representation, this corresponds\n",
    "to an empty dictionary. $\\textbf{Note:}$ With this problem, you will\n",
    "need to take some care to code things efficiently. In particular,\n",
    "be aware that making copies of the weight dictionary can slow down\n",
    "your code significantly. If you want to make a copy of your weights\n",
    "(e.g. for checking for convergence), make sure you don't do this more\n",
    "than once per epoch. $\\textbf{Also}$: If you normalize your data in\n",
    "some way, be sure not to destroy the sparsity of your data. Anything\n",
    "that starts as $0$ should stay at $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Implement the Pegasos algorithm\n",
    "import numpy as np\n",
    "from util import dotProduct, increment, scaleProduct\n",
    "\n",
    "def svmPegasos(train_examples, regularized_term = 1.0, num_epoch = 10):\n",
    "    num_train = len(train_examples)\n",
    "    w = Counter()\n",
    "    t = 0\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        # shuffle training data\n",
    "        np.random.shuffle(train_examples)\n",
    "        for idx in range(num_train):\n",
    "            t = t + 1\n",
    "            eta = 1.0/(t*regularized_term)\n",
    "            xi = convertDoc2Sparse(train_examples[idx][:-1])\n",
    "            yi = train_examples[idx][-1]\n",
    "            margin = yi * dotProduct(w, xi)\n",
    "            scaleProduct(w, 1-eta*regularized_term)\n",
    "            if margin < 1:\n",
    "                increment(w, eta*yi, xi)\n",
    "        print(\"Running epoch # {} in Pegasos algorithm\".format(epoch))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Note that in every step of the Pegasos algorithm, we rescale every\n",
    "entry of $w_{t}$ by the factor $(1-\\eta_{t}\\lambda)$. Implementing\n",
    "this directly with dictionaries is very slow. We can make things significantly\n",
    "faster by representing $w$ as $w=sW$, where $s\\in\\Re$ and $W\\in\\Re^{d}$.\n",
    "You can start with $s=1$ and $W$ all zeros (i.e. an empty dictionary).\n",
    "Note that both updates (i.e. whether or not we have a margin error)\n",
    "start with rescaling $w_{t}$, which we can do simply by setting $s_{t+1}=\\left(1-\\eta_{t}\\lambda\\right)s_{t}$.\n",
    "If the update is $w_{t+1}=(1-\\eta_{t}\\lambda)w_{t}+\\eta_{t}y_{j}x_{j}$,\n",
    "then $\\textbf{verify that the Pegasos update step is equivalent to}$:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "s_{t+1} & = & \\left(1-\\eta_{t}\\lambda\\right)s_{t}\\\\\n",
    "W_{t+1} & = & W_{t}+\\frac{1}{s_{t+1}}\\eta_{t}y_{j}x_{j}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "There is one subtle issue with the approach described above: if we\n",
    "ever have $1-\\eta_{t}\\lambda=0$, then $s_{t+1}=0$, and we'll have\n",
    "a divide by $0$ in the calculation for $W_{t+1}$. This only happens\n",
    "when $\\eta_{t}=1/\\lambda$. With our step-size rule of $\\eta_{t}=1/\\left(\\lambda t\\right)$,\n",
    "it happens exactly when $t=1$. So one approach is to just start at\n",
    "$t=2$. More generically, note that if $s_{t+1}=0$, then $w_{t+1}=0$.\n",
    "Thus an equivalent representation is $s_{t+1}=1$ and $W=0$. Thus\n",
    "if we ever get $s_{t+1}=0$, simply set it back to $1$ and reset\n",
    "$W_{t+1}$ to zero, which is an empty dictionary in a sparse representation.\n",
    "$\\textbf{Implement the Pegasos algorithm with the $(s,W)$ representation\n",
    "described above}$. (See section 5.1 of Leon Bottou's\n",
    "[Stochastic Gradient Tricks](http://leon.bottou.org/papers/bottou-tricks-2012)\n",
    "for a more generic version of this technique, and many other useful\n",
    "tricks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Pegasos algorithm with the (s,W) representation.\n",
    "import numpy as np\n",
    "from util import dotProduct, increment, scaleProduct\n",
    "\n",
    "def svmPegasosV2(train_examples, regularized_term = 1.0, num_epoch = 10):\n",
    "    num_train = len(train_examples)\n",
    "    W = Counter()\n",
    "    t, s = 0, 1\n",
    "    \n",
    "    for epoch in range(num_epoch):   \n",
    "        # shuffle training data\n",
    "        np.random.shuffle(train_examples)\n",
    "        for idx in range(num_train):\n",
    "            t = t + 1\n",
    "            eta = 1.0/(t*regularized_term)\n",
    "            s_next = (1-eta*regularized_term)* s\n",
    "            if s_next == 0:\n",
    "                s = 1\n",
    "                W = Counter()\n",
    "                continue\n",
    "            xi = convertDoc2Sparse(train_examples[idx][:-1])\n",
    "            yi = train_examples[idx][-1]\n",
    "            margin = yi * s * dotProduct(W, xi)\n",
    "            if margin < 1:\n",
    "                increment(W, 1/s*eta*yi, xi)\n",
    "            s = s_next\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"Run epoch# {}/{} in fast Pegasos algorithm with regularized term {}\".format(\n",
    "                epoch, num_epoch, regularized_term))\n",
    "    scaleProduct(W, s) # w = sW\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Run both implementations of Pegasos on the training data for a couple\n",
    "epochs (using the bag-of-words feature representation described above).\n",
    "Make sure your implementations are correct by verifying that the two\n",
    "approaches give essentially the same result. Report on the time taken\n",
    "to run each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run epoch# 0/300 in fast Pegasos algorithm with regularized term 0.2\n",
      "Run epoch# 50/300 in fast Pegasos algorithm with regularized term 0.2\n",
      "Run epoch# 100/300 in fast Pegasos algorithm with regularized term 0.2\n",
      "Run epoch# 150/300 in fast Pegasos algorithm with regularized term 0.2\n",
      "Run epoch# 200/300 in fast Pegasos algorithm with regularized term 0.2\n",
      "Run epoch# 250/300 in fast Pegasos algorithm with regularized term 0.2\n",
      "Total Pegasos algorithm V2 execution time: 205.28203797340393s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Run the first Pegasos algorithm approach\n",
    "# start_time = time.time()\n",
    "# w1 = svmPegasos(train_examples, regularized_term = 1.0, num_epoch = 1)\n",
    "# print(\"Total Pegasos algorithm execution time: {}s\".format(time.time() - start_time))\n",
    "# print(len(w1.keys()))\n",
    "\n",
    "# Run the second Pegassos algorithm approach with the (s, W ) representation\n",
    "start_time = time.time()\n",
    "w2 = svmPegasosV2(train_examples, regularized_term = 0.2, num_epoch = 300)\n",
    "print(\"Total Pegasos algorithm V2 execution time: {}s\".format(time.time() - start_time))\n",
    "# print(len(w2.keys()))\n",
    "\n",
    "# print(w1)\n",
    "# print(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write a function that takes a sparse weight vector $w$ and a collection\n",
    "of $(x,y)$ pairs, and returns the percent error when predicting $y$\n",
    "using $\\text{sign}(w^{T}x)$. In other words, the function reports the 0-1\n",
    "loss of the linear predictor $x\\mapsto w^{T}x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.152\n"
     ]
    }
   ],
   "source": [
    "# Predict the percent error\n",
    "from util import dotProduct\n",
    "\n",
    "def predictPercentError(data, w):\n",
    "    error_count, total_count = 0, len(data)\n",
    "    for idx in range(total_count):\n",
    "        xi = convertDoc2Sparse(data[idx][:-1])\n",
    "        yi = data[idx][-1]\n",
    "        if yi * dotProduct(w, xi) < 0: # make wrong prediction\n",
    "            error_count += 1\n",
    "    return error_count/total_count \n",
    "\n",
    "# Verify on validation set\n",
    "errorRate = predictPercentError(valid_examples, w2)\n",
    "print(errorRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Using the bag-of-words feature representation described above, search\n",
    "for the regularization parameter that gives the minimal percent error\n",
    "on your test set. (You should now use your faster Pegasos implementation,\n",
    "and run it to convergence.) A good search strategy is to start with\n",
    "a set of regularization parameters spanning a broad range of orders\n",
    "of magnitude. Then, continue to zoom in until you're convinced that\n",
    "additional search will not significantly improve your test performance.\n",
    "Once you have a sense of the general range of regularization parameters\n",
    "that give good results, you do not have to search over orders of magnitude\n",
    "every time you change something (such as adding a new feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run epoch# 0/200 in fast Pegasos algorithm with regularized term 100\n",
      "Run epoch# 50/200 in fast Pegasos algorithm with regularized term 100\n",
      "Run epoch# 100/200 in fast Pegasos algorithm with regularized term 100\n",
      "Run epoch# 150/200 in fast Pegasos algorithm with regularized term 100\n",
      "Run epoch# 0/200 in fast Pegasos algorithm with regularized term 10\n",
      "Run epoch# 50/200 in fast Pegasos algorithm with regularized term 10\n",
      "Run epoch# 100/200 in fast Pegasos algorithm with regularized term 10\n",
      "Run epoch# 150/200 in fast Pegasos algorithm with regularized term 10\n",
      "Run epoch# 0/200 in fast Pegasos algorithm with regularized term 1\n",
      "Run epoch# 50/200 in fast Pegasos algorithm with regularized term 1\n",
      "Run epoch# 100/200 in fast Pegasos algorithm with regularized term 1\n",
      "Run epoch# 150/200 in fast Pegasos algorithm with regularized term 1\n",
      "Run epoch# 0/200 in fast Pegasos algorithm with regularized term 0.5\n",
      "Run epoch# 50/200 in fast Pegasos algorithm with regularized term 0.5\n",
      "Run epoch# 100/200 in fast Pegasos algorithm with regularized term 0.5\n",
      "Run epoch# 150/200 in fast Pegasos algorithm with regularized term 0.5\n",
      "Run epoch# 0/200 in fast Pegasos algorithm with regularized term 0.2\n",
      "Run epoch# 50/200 in fast Pegasos algorithm with regularized term 0.2\n",
      "Run epoch# 100/200 in fast Pegasos algorithm with regularized term 0.2\n",
      "Run epoch# 150/200 in fast Pegasos algorithm with regularized term 0.2\n",
      "Run epoch# 0/200 in fast Pegasos algorithm with regularized term 0.1\n",
      "Run epoch# 50/200 in fast Pegasos algorithm with regularized term 0.1\n",
      "Run epoch# 100/200 in fast Pegasos algorithm with regularized term 0.1\n",
      "Run epoch# 150/200 in fast Pegasos algorithm with regularized term 0.1\n",
      "Run epoch# 0/200 in fast Pegasos algorithm with regularized term 0.05\n",
      "Run epoch# 50/200 in fast Pegasos algorithm with regularized term 0.05\n",
      "Run epoch# 100/200 in fast Pegasos algorithm with regularized term 0.05\n",
      "Run epoch# 150/200 in fast Pegasos algorithm with regularized term 0.05\n",
      "Run epoch# 0/200 in fast Pegasos algorithm with regularized term 0.01\n",
      "Run epoch# 50/200 in fast Pegasos algorithm with regularized term 0.01\n",
      "Run epoch# 100/200 in fast Pegasos algorithm with regularized term 0.01\n",
      "Run epoch# 150/200 in fast Pegasos algorithm with regularized term 0.01\n",
      "Run epoch# 0/200 in fast Pegasos algorithm with regularized term 0.001\n",
      "Run epoch# 50/200 in fast Pegasos algorithm with regularized term 0.001\n",
      "Run epoch# 100/200 in fast Pegasos algorithm with regularized term 0.001\n",
      "Run epoch# 150/200 in fast Pegasos algorithm with regularized term 0.001\n",
      "regularized paramter 100: error rate is 0.478\n",
      "regularized paramter 10: error rate is 0.304\n",
      "regularized paramter 1: error rate is 0.166\n",
      "regularized paramter 0.5: error rate is 0.168\n",
      "regularized paramter 0.2: error rate is 0.156\n",
      "regularized paramter 0.1: error rate is 0.158\n",
      "regularized paramter 0.05: error rate is 0.17\n",
      "regularized paramter 0.01: error rate is 0.166\n",
      "regularized paramter 0.001: error rate is 0.176\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter search\n",
    "\n",
    "regularized_term_list = [10**2, 10, 1, 0.5, 0.2, 0.1, 0.05, 10**-2, 10**-3]\n",
    "# regularized_term_list = [1]\n",
    "error_rate_list = []\n",
    "\n",
    "for r in regularized_term_list:\n",
    "    w = svmPegasosV2(train_examples, regularized_term = r, num_epoch = 200)\n",
    "    error_rate = predictPercentError(valid_examples, w)\n",
    "    error_rate_list.append(error_rate)\n",
    "\n",
    "# Display result\n",
    "for i in range(len(regularized_term_list)):\n",
    "    print(\"regularized paramter {}: error rate is {}\".format(\n",
    "        regularized_term_list[i], error_rate_list[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Error Analysis\n",
    "\n",
    "The natural language processing domain is particularly nice in that one can often interpret why a model has performed well or poorly on a specific example, and sometimes it is not very difficult to come up with ideas for new features that might help fix a problem. The\n",
    "first step in this process is to look closely at the errors that our\n",
    "model makes.\n",
    "\n",
    "1. Choose an input example $x=\\left(x_{1},\\ldots,x_{d}\\right)\\in\\Re^{d}$\n",
    "that the model got wrong. We want to investigate what features contributed\n",
    "to this incorrect prediction. One way to rank the importance of the\n",
    "features to the decision is to sort them by the size of their contributions\n",
    "to the score. That is, for each feature we compute $\\left|w_{i}x_{i}\\right|$,\n",
    "where $w_{i}$ is the weight of the $i$th feature in the prediction\n",
    "function, and $x_{i}$ is the value of the $i$th feature in the input\n",
    "$x$. Create a table of the most important features, sorted by $\\left|w_{i}x_{i}\\right|$,\n",
    "including the feature name, the feature value $x_{i}$, the feature\n",
    "weight $w_{i}$, and the product $w_{i}x_{i}$. Attempt to explain\n",
    "why the model was incorrect. Can you think of a new feature that might\n",
    "be able to fix the issue? Include a short analysis for at least 2\n",
    "incorrect examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first example is -1 label, predicted as 2.0514856620204314\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>wi</th>\n",
       "      <th>wixi</th>\n",
       "      <th>wixi_abs</th>\n",
       "      <th>xi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>and</td>\n",
       "      <td>0.025845</td>\n",
       "      <td>1.085489</td>\n",
       "      <td>1.085489</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the</td>\n",
       "      <td>0.009806</td>\n",
       "      <td>0.902166</td>\n",
       "      <td>0.902166</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i</td>\n",
       "      <td>0.018439</td>\n",
       "      <td>0.848181</td>\n",
       "      <td>0.848181</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>have</td>\n",
       "      <td>-0.055104</td>\n",
       "      <td>-0.440833</td>\n",
       "      <td>0.440833</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>why</td>\n",
       "      <td>-0.053836</td>\n",
       "      <td>-0.430686</td>\n",
       "      <td>0.430686</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>see</td>\n",
       "      <td>0.065249</td>\n",
       "      <td>0.391496</td>\n",
       "      <td>0.391496</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>to</td>\n",
       "      <td>-0.012099</td>\n",
       "      <td>-0.362983</td>\n",
       "      <td>0.362983</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>is</td>\n",
       "      <td>0.013218</td>\n",
       "      <td>0.343666</td>\n",
       "      <td>0.343666</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>as</td>\n",
       "      <td>0.033636</td>\n",
       "      <td>0.336362</td>\n",
       "      <td>0.336362</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>so</td>\n",
       "      <td>-0.031281</td>\n",
       "      <td>-0.312813</td>\n",
       "      <td>0.312813</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name        wi      wixi  wixi_abs  xi\n",
       "135   and  0.025845  1.085489  1.085489  42\n",
       "5     the  0.009806  0.902166  0.902166  92\n",
       "4       i  0.018439  0.848181  0.848181  46\n",
       "145  have -0.055104 -0.440833  0.440833   8\n",
       "168   why -0.053836 -0.430686  0.430686   8\n",
       "102   see  0.065249  0.391496  0.391496   6\n",
       "82     to -0.012099 -0.362983  0.362983  30\n",
       "96     is  0.013218  0.343666  0.343666  26\n",
       "119    as  0.033636  0.336362  0.336362  10\n",
       "90     so -0.031281 -0.312813  0.312813  10"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create feature table\n",
    "def createFeatTable(x, w):\n",
    "    table = []\n",
    "    for k,v in x.items():\n",
    "        wi = w.get(k,0)\n",
    "        table.append({'name': k, 'xi': v, 'wi': wi, 'wixi': wi*v, 'wixi_abs': abs(wi*v)})\n",
    "    return table\n",
    "\n",
    "# Choose hyperparameter regularized term 0.2 as to analyze the error by running `svmPegasosV2` above.\n",
    "# Find top 2 index in valid examples having prediction error\n",
    "result = []\n",
    "for i in range(len(valid_examples)):\n",
    "    xi = convertDoc2Sparse(valid_examples[i][:-1])\n",
    "    yi = valid_examples[i][-1]\n",
    "    result.append({'val': yi * dotProduct(w2, xi), 'index': i})\n",
    "result.sort(key=lambda x: x['val'])\n",
    "# print(result)\n",
    "\n",
    "# Find top two errors\n",
    "# The first error\n",
    "idx1 = result[0]['index']\n",
    "x1 = convertDoc2Sparse(valid_examples[idx1][:-1])\n",
    "y1 = valid_examples[idx1][-1]\n",
    "print('The first example is {} label, predicted as {}'.format(y1, dotProduct(w2, x1)))\n",
    "t1 = createFeatTable(x1, w2)\n",
    "data1 = pd.DataFrame(t1)\n",
    "data1 = data1.sort_values(by='wixi_abs', ascending=False)\n",
    "data1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We only consider word count as features, the word 'and' appears two many times and the learned $w_i$ is positive which pulls $w_i x_i$ to a positive large number. We can also see similar results on word 'the'. We might try to normalize the work count for each word to fix this issue or use tf-idf as feature instead.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second example is 1 label, predicted as -1.4796341005594358\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>wi</th>\n",
       "      <th>wixi</th>\n",
       "      <th>wixi_abs</th>\n",
       "      <th>xi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>as</td>\n",
       "      <td>0.033636</td>\n",
       "      <td>0.403635</td>\n",
       "      <td>0.403635</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>the</td>\n",
       "      <td>0.009806</td>\n",
       "      <td>0.343215</td>\n",
       "      <td>0.343215</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>even</td>\n",
       "      <td>-0.051443</td>\n",
       "      <td>-0.308656</td>\n",
       "      <td>0.308656</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>and</td>\n",
       "      <td>0.025845</td>\n",
       "      <td>0.284295</td>\n",
       "      <td>0.284295</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>!</td>\n",
       "      <td>-0.030849</td>\n",
       "      <td>-0.277642</td>\n",
       "      <td>0.277642</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>any</td>\n",
       "      <td>-0.070071</td>\n",
       "      <td>-0.210212</td>\n",
       "      <td>0.210212</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>worst</td>\n",
       "      <td>-0.104383</td>\n",
       "      <td>-0.208765</td>\n",
       "      <td>0.208765</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>into</td>\n",
       "      <td>-0.042721</td>\n",
       "      <td>-0.170884</td>\n",
       "      <td>0.170884</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>i</td>\n",
       "      <td>0.018439</td>\n",
       "      <td>0.165949</td>\n",
       "      <td>0.165949</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>are</td>\n",
       "      <td>-0.031585</td>\n",
       "      <td>-0.157923</td>\n",
       "      <td>0.157923</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name        wi      wixi  wixi_abs  xi\n",
       "33      as  0.033636  0.403635  0.403635  12\n",
       "70     the  0.009806  0.343215  0.343215  35\n",
       "48    even -0.051443 -0.308656  0.308656   6\n",
       "36     and  0.025845  0.284295  0.284295  11\n",
       "91       ! -0.030849 -0.277642  0.277642   9\n",
       "38     any -0.070071 -0.210212  0.210212   3\n",
       "81   worst -0.104383 -0.208765  0.208765   2\n",
       "148   into -0.042721 -0.170884  0.170884   4\n",
       "51       i  0.018439  0.165949  0.165949   9\n",
       "63     are -0.031585 -0.157923  0.157923   5"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The second error\n",
    "idx2 = result[1]['index']\n",
    "x2 = convertDoc2Sparse(valid_examples[idx2][:-1])\n",
    "y2 = valid_examples[idx2][-1]\n",
    "print('The second example is {} label, predicted as {}'.format(y2, dotProduct(w2, x2)))\n",
    "t2 = createFeatTable(x2, w2)\n",
    "data2 = pd.DataFrame(t2)\n",
    "data2 = data2.sort_values(by='wixi_abs', ascending=False)\n",
    "data2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: We can see the word 'worst' with a high weight learned from SVM to heavily pull down the predicted value. This is expected since we usually think that the movie will be negative if we see word 'worst'. But we have not analyzed the semantics and do not know if the word indeed is 'not worst'. To fix this issue, we can try to add bi-gram model feature, i.e., we not only analyze word itself, but also we should analyze two continous words in setences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Features\n",
    "\n",
    "For a problem like this, the features you use are far more important\n",
    "than the learning model you choose. Whenever you enter a new problem\n",
    "domain, one of your first orders of business is to beg, borrow, or\n",
    "steal the best features you can find. This means looking at any relevant\n",
    "published work and seeing what they've used. Maybe it means asking\n",
    "a colleague what features they use. But eventually you'll need to\n",
    "engineer new features that help in your particular situation. To get\n",
    "ideas for this dataset, you might check the discussion board on this \n",
    "[Kaggle competition](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews),\n",
    "which is using a very similar dataset. There are also a very large\n",
    "number of academic research papers on sentiment analysis that you\n",
    "can look at for ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
