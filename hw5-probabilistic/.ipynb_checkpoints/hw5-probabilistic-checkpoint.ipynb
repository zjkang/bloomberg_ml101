{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Conditional Probability Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this homework we'll be investigating conditional probability models,\n",
    "with a focus on various interpretations of logistic regression, with\n",
    "and without regularization. Along the way we'll discuss the calibration\n",
    "of probability predictions, both in the limit of infinite training\n",
    "data and in a more bare-hands way. On the Bayesian side, we'll recreate\n",
    "from scratch the Bayesian linear gaussian regression example we discussed\n",
    "in lecture. We'll also have several optional problems that work through\n",
    "many basic concepts in Bayesian statistics via one of the simplest\n",
    "problems there is: estimating the probability of heads in a coin flip.\n",
    "Later we'll extend this to the probability of estimating click-through\n",
    "rates in mobile advertising. Along the way we'll encounter empirical\n",
    "Bayes and hierarchical models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. From Scores to Conditional Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the classification setting, in which $\\left(x_{1},y_{1}\\right),\\ldots,\\left(x_{n},y_{n}\\right)\\in\\mathcal{X}\\times\\left\\{ -1,1\\right\\} $ are sampled i.i.d. from some unknown distribution. For a prediction\n",
    "function $f:\\mathcal{X}\\to\\Re$, we define the \\textbf{margin }on an example\n",
    "$\\left(x,y\\right)$ to be $m=yf(x)$. Since our class predictions\n",
    "are given by $\\textrm{sign}(f(x))$, we see that a prediction is correct iff\n",
    "$m(x)>0$. It's tempting to interpret the magnitude of the score $\\left|f(x)\\right|$\n",
    "as a measure of confidence. However, it's hard to interpret the magnitudes\n",
    "beyond saying one prediction score is more or less confident than\n",
    "another, and without any scale to this ``confidence score``, it's\n",
    "hard to know what to do with it. In this problem, we investigate how\n",
    "we can translate the score into a probability, which is much easier\n",
    "to interpret. In other words, we are looking for a way to convert\n",
    "score function $f(x)\\in\\Re$ into a conditional probability distribution\n",
    "$x\\mapsto p(y=1\\mid x)$. \n",
    "\n",
    "In this problem we will consider $\\textbf{margin-based losses}$, which\n",
    "are loss functions of the form $\\left(y,f(x)\\right)\\mapsto\\ell\\left(yf(x)\\right)$,\n",
    "where $m=yf(x)$ is called the $\\textbf{margin}$. We are interested\n",
    "in how we can go from an empirical risk minimizer for a margin-based\n",
    "loss, $\\hat{f}=\\text{argmin}_{f\\in\\mathcal{F}}\\sum_{i=1}^{n}\\ell\\left(y_{i}f(x_{i})\\right)$,\n",
    "to a conditional probability estimator $\\hat{\\pi}(x)\\approx p(y=1\\mid x)$.\n",
    "Our approach will be to try to find a way to use the Bayes prediction function $f^{*}=\\text{argmin}_{f}\\mathbb{E}_{x,y}\\left[\\ell(yf(x)\\right]$ to get the true\n",
    "conditional probability $\\pi(x)=p(y=1\\mid x$), and then apply the\n",
    "same mapping to the empirical risk minimizer. While there is plenty\n",
    "that can go wrong with this ``plug-in`` approach (primarily, the\n",
    "empirical risk minimizer from a limited hypothesis space $\\mathcal{F}$\n",
    "may be a poor estimate for the Bayes prediction function), it is at\n",
    "least well-motivated, and it can work well in practice. And $\\textbf{please\n",
    "note}$ that we can do better than just hoping for success: if you have\n",
    "enough validation data, you can directly assess how well ``calibrated``\n",
    "the predicted probabilities are. This blog post has some discussion\n",
    "of calibration plots: [https://jmetzen.github.io/2015-04-14/calibration.html](https://jmetzen.github.io/2015-04-14/calibration.html). \n",
    "\n",
    "It turns out it is straightforward to find the Bayes prediction function\n",
    "$f^{*}$ for margin losses, at least in terms of the data-generating\n",
    "distribution: For any given $x\\in\\mathcal{X}$, we'll find the best possible\n",
    "prediction $\\hat{y}$. This will be the $\\hat{y}$ that minimizes\n",
    "$$\n",
    "\\mathbb{E}_{y}\\left[\\ell\\left(y\\hat{y}\\right)\\mid x\\right].\n",
    "$$\n",
    "If we can calculate this $\\hat{y}$ for all $x\\in\\mathcal{X}$, then we will\n",
    "have determined $f^{*}(x)$. We will simply take\n",
    "$$\n",
    "f^{*}(x)=\\text{argmin}_{\\hat{y}}\\mathbb{E}_{y}\\left[\\ell\\left(y\\hat{y}\\right)\\mid x\\right].\n",
    "$$\n",
    "\n",
    "Below we'll calculate $f^{*}$ for several loss functions. It will\n",
    "be convenient to let $\\pi(x)=p\\left(y=1\\mid x\\right)$ in the work\n",
    "below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Write $\\mathbb{E}{y}\\left[\\ell\\left(yf(x)\\right)\\mid x\\right]$ in terms\n",
    "of $\\pi(x)$, $\\ell(-f(x))$, and $\\ell\\left(f(x)\\right)$. (Hint:\n",
    "Use the fact that $y\\in\\left\\{ -1,1\\right\\}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\\begin{eqnarray*}\n",
    "\\mathbb{E}{y}\\left[\\ell\\left(yf(x)\\right)\\mid x\\right]\n",
    "& = & \\ell\\left(f(x)\\right) p(y=1\\mid x) + \\ell\\left(-f(x)\\right) p(y=-1\\mid x) \\\\\n",
    "& = & \\ell\\left(f(x)\\right) p(y=1\\mid x) + \\ell\\left(-f(x)\\right) (1-p(y=1\\mid x)) \\\\\n",
    "& = & \\ell\\left(f(x)\\right) \\pi(x) + \\ell\\left(-f(x)\\right) (1-\\pi(x))\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Show that the Bayes prediction function $f^{*}(x)$ for the exponential\n",
    "loss function $\\ell\\left(y,f(x)\\right)=e^{-yf(x)}$ is given by \n",
    "$$\n",
    "f^{*}(x)=\\frac{1}{2}\\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right),\n",
    "$$\n",
    "where we've assumed $\\pi(x)\\in\\left(0,1\\right)$. Also, show that\n",
    "given the Bayes prediction function $f^{*}$, we can recover the conditional\n",
    "probabilities by\n",
    "$$\n",
    "\\pi(x)=\\frac{1}{1+e^{-2f^{*}(x)}}.\n",
    "$$\n",
    "(Hint: Differentiate the expression in the previous problem with\n",
    "respect to $f(x)$. To make things a little less confusing, and also\n",
    "to write less, you may find it useful to change variables a bit: Fix\n",
    "an $x\\in\\mathcal{X}$. Then write $p=\\pi(x)$ and $\\hat{y}=f(x)$. After substituting\n",
    "these into the expression you had for the previous problem, you'll\n",
    "want to find $\\hat{y}$ that minimizes the expression. Use differential\n",
    "calculus. Once you've done it for a single $x$, it's easy to write\n",
    "the solution as a function of $x$.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Let's write $p=\\pi(x)$ and $\\hat{y}=f(x)$, then we have\n",
    "$$\n",
    "\\frac{\\partial \\ell\\left(y,f(x)\\right)}{\\partial \\hat{y}}\n",
    "= e^{-y \\hat{y}} (-y)\n",
    "$$\n",
    "Then we can calculate the partial derivative with respect to expectation function,\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial \\mathbb{E}}{\\partial \\hat{y}}\n",
    "& = & -e^{-\\hat{y}} p + e^{\\hat{y}} (1-p) \\\\\n",
    "& = & 0\n",
    "\\end{eqnarray*}\n",
    "By calculating $\\hat{y}$, and replace $p$ with $\\pi(x)$ we arrive at\n",
    "$$\n",
    "f^{*}(x)=\\frac{1}{2}\\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right),\n",
    "$$\n",
    "From the above equation and arrange the equation, we can find $\\pi(x)$ as\n",
    "\\begin{eqnarray*}\n",
    "\\pi(x) & = & \\frac{e^{\\hat{y}}}{e^{-\\hat{y}} + e^{\\hat{y}}} \\\\\n",
    "& = & \\frac{1}{1+e^{-2f^{*}(x)}}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Show that the Bayes prediction function $f^{*}(x)$ for the logistic\n",
    "loss function $\\ell\\left(y,f(x)\\right)=\\ln\\left(1+e^{-yf(x)}\\right)$\n",
    "is given by\n",
    "$$\n",
    "f^{*}(x)=\\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)\n",
    "$$\n",
    "and the conditional probabilities are given by\n",
    "$$\n",
    "\\pi(x)=\\frac{1}{1+e^{-f^{*}(x)}}.\n",
    "$$\n",
    "Again, we may assume that $\\pi(x)\\in(0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Let's write $p=\\pi(x)$ and $\\hat{y}=f(x)$, then we have\n",
    "$$\n",
    "\\frac{\\partial \\ell\\left(y,f(x)\\right)}{\\partial \\hat{y}}\n",
    "= \\frac{1}{1+e^{-y \\hat{y}}} e^{-y \\hat{y}} (-y)\n",
    "$$\n",
    "Then we can calculate the partial derivative with respect to expectation function,\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial \\mathbb{E}}{\\partial \\hat{y}}\n",
    "& = & \\frac{-e^{-\\hat{y}}}{1+e^{-\\hat{y}}} p + \\frac{e^{\\hat{y}}}{1+e^{\\hat{y}}} (1-p) \\\\\n",
    "& = & 0\n",
    "\\end{eqnarray*}\n",
    "By calculating $\\hat{y}$, and replace $p$ with $\\pi(x)$ we arrive at\n",
    "$$\n",
    "\\frac{1}{1+e^{\\hat{y}}} p = \\frac{1}{1+e^{-\\hat{y}}} (1-p) \\\\\n",
    "f^{*}(x)=\\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)\n",
    "$$\n",
    "From the above equation and arrange the equation, we can find $\\pi(x)$ as\n",
    "$$\n",
    "\\pi(x)=\\frac{1}{1+e^{-f^{*}(x)}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Equivalence of ERM and probabilistic approaches\n",
    "\n",
    "In lecture we discussed two different ways to end up with logistic regression. \n",
    "\n",
    "$\\textbf{ERM approach:}$ Consider the classification setting with input\n",
    "space $\\mathcal{X}=\\Re^{d}$, outcome space $\\mathcal{Y}_{\\pm}=\\left\\{ -1,1\\right\\} $,\n",
    "and action space $\\mathcal{A}_{\\Re}=\\Re$, with the hypothesis space of linear score functions $\\mathcal{F}_{\\text{score}}=\\left\\{ x\\mapsto x^{T}w\\mid w\\in\\Re^{d}\\right\\} $.\n",
    "Consider the margin-based loss function $\\ell_{\\text{logistic}}(m)=\\log\\left(1+e^{-m}\\right)$\n",
    "and the training data $\\mathcal{D}=\\left((x_{1},y_{1}),\\ldots,(x_{n},y_{n})\\right)$.\n",
    "Then the empirical risk objective function for hypothesis space $\\mathcal{F}_{\\text{score}}$\n",
    "and the logistic loss over $\\mathcal{D}$ is given by\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\hat{R}_{n}(w) & = & \\frac{1}{n}\\sum_{i=1}^{n}\\ell_{\\text{logistic}}(y_{i}w^{T}x_{i})\\\\\n",
    " & = & \\frac{1}{n}\\sum_{i=1}^{n}\\log\\left(1+\\exp\\left(-y_{i}w^{T}x_{i}\\right)\\right).\n",
    "\\end{eqnarray*}\n",
    "\n",
    "$\\textbf{Bernoulli regression with logistic transfer function:}$ Consider\n",
    "the conditional probability modeling setting with input space $\\mathcal{X}=\\Re^{d}$,\n",
    "outcome space $\\mathcal{Y}_{0/1}=\\left\\{ 0,1\\right\\} $, and action space\n",
    "$\\mathcal{A}_{[0,1]}=[0,1]$, where an action corresponds to the predicted\n",
    "probability that an outcome is $1$. Define the $\\textbf{standard logistic\n",
    "function}$ as $\\phi(\\eta)=1/\\left(1+e^{-\\eta}\\right)$ and the hypothesis\n",
    "space $\\mathcal{F}_{\\text{prob}}=\\left\\{ x\\mapsto\\phi(w^{T}x)\\mid w\\in\\Re^{d}\\right\\} $.\n",
    "Suppose for every $y_{i}$ in the dataset $\\mathcal{D}$ above, we define\n",
    "$y_{i}'=\\begin{cases}\n",
    "1 & y_{i}=1\\\\\n",
    "0 & y_{i}=-1\n",
    "\\end{cases}$, and let $\\mathcal{D}'$ be the resulting collection of $\\left(x_{i},y_{i}'\\right)$ pairs. Then the negative log-likelihood (NLL) objective function for $\\mathcal{F}_{\\text{prob}}$ and $\\mathcal{D}'$ is give by \n",
    "\\begin{eqnarray*}\n",
    "\\text{NLL}(w) & = & -\\sum_{i=1}^{n}y_{i}'\\log\\phi(w^{T}x_{i})+\\left(1-y_{i}'\\right)\\log\\left(1-\\phi(w^{T}x_{i})\\right)\\\\\n",
    " & = & \\sum_{i=1}^{n}\\left[-y_{i}'\\log\\phi(w^{T}x_{i})\\right]+\\left(y_{i}'-1\\right)\\log\\left(1-\\phi(w^{T}x_{i})\\right)\n",
    "\\end{eqnarray*}\n",
    "If $\\hat{w}_{\\text{prob}}$ minimizes $\\text{NLL}(w)$, then $x\\mapsto\\phi(x^{T}\\hat{w}_{\\text{prob}})$\n",
    "is a maximum likelihood prediction function over the hypothesis space\n",
    "$\\mathcal{F}_{\\text{prob}}$ for the dataset $\\mathcal{D}'$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.1** $\\textbf{Show that}$ $n\\hat{R}_{n}(w)=\\text{NLL}(w)$ for all $w\\in\\Re^{d}$.\n",
    "And thus the two approaches are equivalent, in that they produce the\n",
    "same prediction functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: We are going to show two approaches are equivalent by comparing one training sample, and can natually extend to all training samples.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\text{NLL}(w) & = & -(y_{i}'\\log\\phi(w^{T}x_{i})+\\left(1-y_{i}'\\right)\\log\\left(1-\\phi(w^{T}x_{i})\\right))\\\\\n",
    "& = & - \\log (\\phi(w^{T}x_{i})^{y_{i}'} \\left(1-\\phi(w^{T}x_{i})\\right)^{1-y_{i}'} )\n",
    "\\end{eqnarray*}\n",
    "- when $y_{i}'=1$, $\\text{NLL}(w) = -\\log (\\phi(w^{T}x_{i}))$, \n",
    "- which is the same as $n\\hat{R}_{n}(w)$; when $y_{i}'=0$, $\\text{NLL}(w) = -\\log (1-\\phi(w^{T}x_{i}))$, which is the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Numerical Overflow and the log-sum-exp trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to calculate $\\log\\left(\\exp(\\eta)\\right)$ for $\\eta=1000.42$.\n",
    "If we compute this literally in Python, we will get an overflow (try\n",
    "it!), since numpy gets infinity for $e^{1000.42}$, and log of infinity\n",
    "is still infinity. On the other hand, we can help out with some math:\n",
    "obviously $\\log\\left(\\exp(\\eta)\\right)=\\eta$, and there's no issue. \n",
    "\n",
    "It turns out, $\\log(\\exp(\\eta))$ and the problem with its calculation\n",
    "is a special case of the [LogSumExp](https://en.wikipedia.org/wiki/LogSumExp)\n",
    "function that shows up frequently in machine learning. We define\n",
    "$$\n",
    "\\text{LogSumExp}(x_{1},\\ldots,x_{n})=\\log\\left(e^{x_{1}}+\\cdots+e^{x_{n}}\\right).\n",
    "$$\n",
    "\n",
    "Note that this will overflow if any of the $x_{i}$'s are large (more\n",
    "than 709). To compute this on a computer, we can use the $\\textbf{log-sum-exp\n",
    "trick}$. We let $x^{*}=\\max\\left(x_{1},\\ldots,x_{n}\\right)$ and\n",
    "compute LogSumExp as\n",
    "$$\n",
    "\\text{LogSumExp}(x_{1},\\ldots,x_{n})=x^{*}+\\log\\left[e^{x_{1}-x^{*}}+\\cdots+e^{x_{n}-x^{*}}\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.1** Show that the new expression for LogSumExp is valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: We show that \n",
    "\\begin{eqnarray*}\n",
    "\\text{LogSumExp}(x_{1},\\ldots,x_{n}) \n",
    "&=& \\log\\left(e^{x_{1}}+\\cdots+e^{x_{n}}\\right) \\\\\n",
    "&=& \\log\\left(x^{*} \\frac{e^{x_{1}}+\\cdots+e^{x_{n}}}{x^{*}} \\right) \\\\\n",
    "&=& x^{*}+\\log\\left[e^{x_{1}-x^{*}}+\\cdots+e^{x_{n}-x^{*}}\\right]\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.2**: Show that $\\exp\\left(x_{i}-x^{*}\\right)\\in(0,1]$ for any $i$, and\n",
    "thus the exp calculations will not overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Since $x_{i}-x^{*} <= 0$, thus $\\exp\\left(x_{i}-x^{*}\\right)\\in(0,1]$, making the exp calculations never go overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.3**: Above we've only spoken about the exp overflowing. However, the log\n",
    "part can also have problems by becoming negative infinity for arguments\n",
    "very close to $0$. Explain why the $\\log$ term in our expression\n",
    "$\\log\\left[e^{x_{1}-x^{*}}+\\cdots+e^{x_{n}-x^{*}}\\right]$ will never\n",
    "be ``-inf``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: since $x^{*}$ is the maxmum values all all x_i's, meaning that $\\left[e^{x_{1}-x^{*}}+\\cdots+e^{x_{n}-x^{*}}\\right]$ always larger than $1$. Then we have never have no issue ``-inf``. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.4**: In the objective functions for logistic regression, there are expressions\n",
    "of the form $\\log\\left(1+e^{-s}\\right)$ for some $s$. Note that a naive implementation gives $0$ for $s>36$ and inf for $s<-709$.\n",
    "Show how to use the numpy function [logaddexp](https://docs.scipy.org/doc/numpy/reference/generated/numpy.logaddexp.html)\n",
    "to correctly compute $\\log\\left(1+e^{-s}\\right)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.319522830243569e-16\n",
      "709.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "s = 36\n",
    "print(np.logaddexp(0, -s))\n",
    "s = -709\n",
    "print(np.logaddexp(0, -s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Regularized Logistic Regression\n",
    "\n",
    "For a dataset $\\mathcal{D}=\\left((x_{1},y_{1}),\\ldots,(x_{n},y_{n})\\right)$\n",
    "drawn from $\\Re^{d}\\times\\left\\{ -1,1\\right\\} $, the regularized\n",
    "logistic regression objective function can be defined as\n",
    "\\begin{eqnarray*}\n",
    "J_{\\text{logistic}}(w) & = & \\hat{R}_{n}(w)+\\lambda\\|w\\|^{2}\\\\\n",
    " & = & \\frac{1}{n}\\sum_{i=1}^{n}\\log\\left(1+\\exp\\left(-y_{i}w^{T}x_{i}\\right)\\right)+\\lambda\\|w\\|^{2}.\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.1**: Prove that the objective function $J_{\\text{logistic}}(w)$ is convex.\n",
    "You may use any facts mentioned in the [convex optimization notes](https://davidrosenberg.github.io/mlcourse/Notes/convex-optimization.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.2**: Complete the $\\text{f_objective}$ function in the skeleton code,\n",
    "which computes the objective function for $J_{\\text{logistic}}(w)$.\n",
    "Make sure to use the log-sum-exp trick to get accurate calculations and to prevent overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.3**: Complete the $\\text{fit_logistic_regression_function}$ in the skeleton\n",
    "code using the $\\texttt{minimize}$ function from $\\texttt{scipy.optimize}$.\n",
    "$\\text{ridge_regression.py}$ from Homework 2 gives an example of\n",
    "how to use the $\\texttt{minimize}$ function. Use this function to train\n",
    "a model on the provided data. Make sure to take the appropriate preprocessing\n",
    "steps, such as standardizing the data and adding a column for the\n",
    "bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.4**: Find the $\\ell_{2}$ regularization parameter that minimizes the log-likelihood\n",
    "on the validation set. Plot the log-likelihood for different values\n",
    "of the regularization parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.5**: Based on the Bernoulli regression development of logistic regression,\n",
    "it seems reasonable to interpret the prediction $f(x)=\\phi(w^{T}x)=1/\\left(1+e^{-w^{T}x}\\right)$\n",
    "as the probability that $y=1$, for a randomly drawn pair $\\left(x,y\\right)$.\n",
    "Since we only have a finite sample (and we are regularizing, which\n",
    "will bias things a bit) there is a question of how well [calibrated](https://en.wikipedia.org/wiki/Calibration_(statistics) \n",
    "our predicted probabilities are. Roughly speaking, we say $f(x)$\n",
    "is well calibrated if we look at all examples $\\left(x,y\\right)$\n",
    "for which $f(x)\\approx0.7$ and we find that close to $70\\%$ of those\n",
    "examples have $y=1$, as predicted... and then we repeat that for\n",
    "all predicted probabilities in $\\left(0,1\\right)$. To see how well-calibrated\n",
    "our predicted probabilities are, break the predictions on the validation\n",
    "set into groups based on the predicted probability (you can play with\n",
    "the size of the groups to get a result you think is informative).\n",
    "For each group, examine the percentage of positive labels. You can\n",
    "make a table or graph. Summarize the results. You may get some ideas\n",
    "and references from [scikit-learn's discussion](http://scikit-learn.org/stable/modules/calibration.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bayesian Logistic Regression with Gaussian Priors\n",
    "\n",
    "Let's return to the setup described in Section 3.1 and, in particular, to the Bernoulli regression setting with logistic transfer function. We had the following hypothesis space of conditional\n",
    "probability functions:\n",
    "$$\n",
    "\\mathcal{F}_{\\text{prob}}=\\left\\{ x\\mapsto\\phi(w^{T}x)\\mid w\\in\\Re^{d}\\right\\} .\n",
    "$$\n",
    "Now let's consider the Bayesian setting, where we induce a prior on\n",
    "$\\mathcal{F}_{\\text{prob}}$ by taking a prior $p(w)$ on the parameter $w\\in\\Re^{d}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** For the dataset $\\mathcal{D}'$ described in Section 3.1,give an expression for the posterior density $p(w\\mid\\mathcal{D}')$ in terms of the negative log-likelihood function \n",
    "\\begin{eqnarray*}\n",
    "\\text{NLL}_{\\mathcal{D}'}(w) & = & -\\sum_{i=1}^{n}y_{i}'\\log\\phi(w^{T}x_{i})+\\left(1-y_{i}'\\right)\\log\\left(1-\\phi(w^{T}x_{i})\\right)\n",
    "\\end{eqnarray*}\n",
    "and a prior density $p(w)$ (up to a proportionality constant is fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2** Suppose we take a prior on $w$ of the form $w\\sim\\mathcal{N}(0,\\Sigma)$.\n",
    "Find a covariance matrix $\\Sigma$ such that MAP estimate for $w$\n",
    "after observing data $\\mathcal{D}'$ is the same as the minimizer of the regularized\n",
    "logistic regression function defined in Section 3.3 (and prove it). (Hint: Consider minimizing the negative log posterior of $w$. Also, remember you can drop any terms from the objective\n",
    "function that don't depend on $w$. Also, you may freely use results of previous problems.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3** In the Bayesian approach, the prior should reflect your beliefs about\n",
    "the parameters before seeing the data and, in particular, should be\n",
    "independent on the eventual size of your dataset. Following this,\n",
    "you choose a prior distribution $w\\sim\\mathcal{N}(0,I)$. For a dataset $\\mathcal{D}$\n",
    "of size $n$, how should you choose $\\lambda$ in our regularized\n",
    "logistic regression objective function so that the minimizer is equal\n",
    "to the mode of the posterior distribution of $w$ (i.e. is equal to\n",
    "the MAP estimator). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Bayesian Linear Regression - Implementation\n",
    "\n",
    "In this problem, we will implement Bayesian Gaussian linear regression,\n",
    "essentially reproducing the example [from lecture](https://davidrosenberg.github.io/mlcourse/Archive/2016/Lectures/13a.bayesian-regression.pdf\\#page=12),\n",
    "which in turn is based on the example in Figure 3.7 of Bishop's Pattern\n",
    "Recognition and Machine Learning (page 155). We've provided plotting\n",
    "functionality in \"support_code.py\". Your task is to complete \"problem.py\". The\n",
    "implementation uses np.matrix objects, and you are welcome to use the np.matrix.getI method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1** Implement likelihood\\_func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2** Implement get\\_posterior\\_params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implment get posterior params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3** Implement get\\_predictive\\_params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement get predictive params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4** Run ``python problem.py`` from inside the Bayesian Regression directory\n",
    "to do the regression and generate the plots. This runs through the\n",
    "regression with three different settings for the prior covariance.\n",
    "You may want to change the default behavior in support\\_code.make\\_plots\n",
    "from plt.show, to saving the plots for inclusion in your homework\n",
    "submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.5** Comment on your results. In particular, discuss how each of the following\n",
    "change with sample size and with the strength of the prior:  (i) the\n",
    "likelihood function, (ii) the posterior distribution, and (iii) the\n",
    "posterior predictive distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.6** Our work above was very much ``full Bayes``, in that rather than\n",
    "coming up with a single prediction function, we have a whole distribution\n",
    "over posterior prediction functions. However, sometimes we want a\n",
    "single prediction function, and a common approach is to use the MAP\n",
    "estimate -- that is, choose the prediction function that has the\n",
    "highest posterior likelihood. As we discussed in class, for this setting,\n",
    "we can get the MAP estimate using ridge regression. Use ridge regression\n",
    "to get the MAP prediction function corresponding to the first prior\n",
    "covariance ($\\Sigma=\\frac{1}{2}I$, per the support code). What value\n",
    "did you use for the regularization coefficient? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
