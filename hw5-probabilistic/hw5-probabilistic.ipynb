{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Conditional Probability Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this homework we'll be investigating conditional probability models,\n",
    "with a focus on various interpretations of logistic regression, with\n",
    "and without regularization. Along the way we'll discuss the calibration\n",
    "of probability predictions, both in the limit of infinite training\n",
    "data and in a more bare-hands way. On the Bayesian side, we'll recreate\n",
    "from scratch the Bayesian linear gaussian regression example we discussed\n",
    "in lecture. We'll also have several optional problems that work through\n",
    "many basic concepts in Bayesian statistics via one of the simplest\n",
    "problems there is: estimating the probability of heads in a coin flip.\n",
    "Later we'll extend this to the probability of estimating click-through\n",
    "rates in mobile advertising. Along the way we'll encounter empirical\n",
    "Bayes and hierarchical models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. From Scores to Conditional Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Write $\\mathbb{E}{y}\\left[\\ell\\left(yf(x)\\right)\\mid x\\right]$ in terms\n",
    "of $\\pi(x)$, $\\ell(-f(x))$, and $\\ell\\left(f(x)\\right)$. (Hint:\n",
    "Use the fact that $y\\in\\left\\{ -1,1\\right\\} $.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Show that the Bayes prediction function $f^{*}(x)$ for the exponential\n",
    "loss function $\\ell\\left(y,f(x)\\right)=e^{-yf(x)}$ is given by \n",
    "\\[\n",
    "f^{*}(x)=\\frac{1}{2}\\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right),\n",
    "\\]\n",
    "where we've assumed $\\pi(x)\\in\\left(0,1\\right)$. Also, show that\n",
    "given the Bayes prediction function $f^{*}$, we can recover the conditional\n",
    "probabilities by\n",
    "\\[\n",
    "\\pi(x)=\\frac{1}{1+e^{-2f^{*}(x)}}.\n",
    "\\]\n",
    "{[}Hint: Differentiate the expression in the previous problem with\n",
    "respect to $f(x)$. To make things a little less confusing, and also\n",
    "to write less, you may find it useful to change variables a bit: Fix\n",
    "an $x\\in\\cx$. Then write $p=\\pi(x)$ and $\\hat{y}=f(x)$. After substituting\n",
    "these into the expression you had for the previous problem, you'll\n",
    "want to find $\\hat{y}$ that minimizes the expression. Use differential\n",
    "calculus. Once you've done it for a single $x$, it's easy to write\n",
    "the solution as a function of $x$.{]} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Show that the Bayes prediction function $f^{*}(x)$ for the logistic\n",
    "loss function $\\ell\\left(y,f(x)\\right)=\\ln\\left(1+e^{-yf(x)}\\right)$\n",
    "is given by\n",
    "$$\n",
    "f^{*}(x)=\\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)\n",
    "$$\n",
    "and the conditional probabilities are given by\n",
    "$$\n",
    "\\pi(x)=\\frac{1}{1+e^{-f^{*}(x)}}.\n",
    "$$\n",
    "Again, we may assume that $\\pi(x)\\in(0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bayesian Logistic Regression with Gaussian Priors\n",
    "\n",
    "Let's return to the setup described in Section 3.1 and, in particular, to the Bernoulli regression setting with logistic transfer function. We had the following hypothesis space of conditional\n",
    "probability functions:\n",
    "$$\n",
    "\\cf_{\\text{prob}}=\\left\\{ x\\mapsto\\phi(w^{T}x)\\mid w\\in\\Re^{d}\\right\\} .\n",
    "$$\n",
    "Now let's consider the Bayesian setting, where we induce a prior on\n",
    "$\\cf_{\\text{prob}}$ by taking a prior $p(w)$ on the parameter $w\\in\\Re^{d}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Bayesian Linear Regression - Implementation\n",
    "\n",
    "In this problem, we will implement Bayesian Gaussian linear regression,\n",
    "essentially reproducing the example [from lecture](https://davidrosenberg.github.io/mlcourse/Archive/2016/Lectures/13a.bayesian-regression.pdf\\#page=12),\n",
    "which in turn is based on the example in Figure 3.7 of Bishop's Pattern\n",
    "Recognition and Machine Learning (page 155). We've provided plotting\n",
    "functionality in \"support_code.py\". Your task is to complete \"problem.py\". The\n",
    "implementation uses np.matrix objects, and you are welcome to use the np.matrix.getI method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Implement likelihood\\_func."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Implement get\\_posterior\\_params."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Implement get\\_predictive\\_params."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Run ``python problem.py`` from inside the Bayesian Regression directory\n",
    "to do the regression and generate the plots. This runs through the\n",
    "regression with three different settings for the prior covariance.\n",
    "You may want to change the default behavior in support\\_code.make\\_plots\n",
    "from plt.show, to saving the plots for inclusion in your homework\n",
    "submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5 Comment on your results. In particular, discuss how each of the following\n",
    "change with sample size and with the strength of the prior:  (i) the\n",
    "likelihood function, (ii) the posterior distribution, and (iii) the\n",
    "posterior predictive distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6 Our work above was very much ``full Bayes``, in that rather than\n",
    "coming up with a single prediction function, we have a whole distribution\n",
    "over posterior prediction functions. However, sometimes we want a\n",
    "single prediction function, and a common approach is to use the MAP\n",
    "estimate -- that is, choose the prediction function that has the\n",
    "highest posterior likelihood. As we discussed in class, for this setting,\n",
    "we can get the MAP estimate using ridge regression. Use ridge regression\n",
    "to get the MAP prediction function corresponding to the first prior\n",
    "covariance ($\\Sigma=\\frac{1}{2}I$, per the support code). What value\n",
    "did you use for the regularization coefficient? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
